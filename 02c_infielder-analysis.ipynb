{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predicting Baseball Hall of Fame Induction\n",
    "\n",
    "###  Analysis 3 of 4 -- Infielders\n",
    "\n",
    "####  Eryk Wdowiak and Ken Hoffman\n",
    "\n",
    "data from Lahman Baseball Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import VotingClassifier ##, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  the little date string\n",
    "dt_str = '2020-08-19a'\n",
    "\n",
    "##  pick a dataframe from pickle files\n",
    "# pitchers_df = pickle.load(open('pitchers-df_'+ dt_str +'.p','rb'))\n",
    "# catchers_df = pickle.load(open('catchers-df_'+ dt_str +'.p','rb'))\n",
    "infielders_df = pickle.load(open('infielders-df_'+ dt_str +'.p','rb'))\n",
    "# outfielders_df = pickle.load(open('outfielders-df_'+ dt_str +'.p','rb'))\n",
    "\n",
    "##  give that dataframe an alias\n",
    "df = infielders_df.copy()\n",
    "\n",
    "##  and toss out Pete Rose\n",
    "df = df[~df['playerID'].str.match('rosepe01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bball_log(x):\n",
    "    warnings.filterwarnings('ignore',category=RuntimeWarning)\n",
    "    return np.where(x<1,0,np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies(x, prefix):\n",
    "    return pd.get_dummies(x, prefix = prefix, drop_first = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improve features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  list to take logs of\n",
    "log_list = ['bG','AB','R','H','2B','3B','HR','RBI','bSB','bCS','BB','SO','HBP','SH',\n",
    "            'bG_ps','AB_ps','R_ps','H_ps','2B_ps','3B_ps','HR_ps',\n",
    "            'RBI_ps','bSB_ps','bCS_ps','BB_ps','SO_ps','HBP_ps','SH_ps',\n",
    "            'fG','PO','A','E','DP','fG_ps','PO_ps','A_ps','E_ps','DP_ps','nu_sns']\n",
    "\n",
    "##  take logs\n",
    "for vbl in log_list:\n",
    "    new = 'ln_'+vbl\n",
    "    df[new] = bball_log(df[vbl])\n",
    "\n",
    "##  years since retirement\n",
    "df['since_lst'] = 2018 - df['lst_sn']\n",
    "df['ln_since'] = np.log(df['since_lst'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  prepare training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  when splitting, how many in test?\n",
    "tts_test_size = 250\n",
    "\n",
    "##  what should be SMOTE's ratio of minority to majority?\n",
    "smote_ratio = 0.125\n",
    "\n",
    "##  random states\n",
    "tts_randm = 19\n",
    "smote_randm = 42\n",
    "\n",
    "##  train_test_split()\n",
    "XX = df.drop(columns=['induct','position'])\n",
    "yy = df['induct']\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, yy, \n",
    "                                                    random_state = tts_randm,\n",
    "                                                    test_size = tts_test_size)\n",
    "\n",
    "##  Fit SMOTE to training data\n",
    "Xs_train = X_train.drop(columns=['playerID','teamID'])\n",
    "Xs_test  = X_test.drop(columns=['playerID','teamID'])\n",
    "X_smote, y_smote = SMOTE(sampling_strategy = smote_ratio,\n",
    "                         random_state = smote_randm).fit_sample(Xs_train, y_train)\n",
    "\n",
    "##  recreate the old layout\n",
    "df_train = X_train.join(y_train)\n",
    "df_test = X_test.join(y_test)\n",
    "df_smote = X_smote.join(y_smote)\n",
    "\n",
    "##  clean up!\n",
    "del XX, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  estimation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  keep a list of models\n",
    "# models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "logit results below on fit to WHOLE dataset\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.088012\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 induct   No. Observations:                  632\n",
      "Model:                          Logit   Df Residuals:                      626\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Wed, 19 Aug 2020   Pseudo R-squ.:                  0.6459\n",
      "Time:                        21:19:28   Log-Likelihood:                -55.623\n",
      "converged:                       True   LL-Null:                       -157.07\n",
      "                                        LLR p-value:                 6.807e-42\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -87.3432     13.663     -6.393      0.000    -114.122     -60.564\n",
      "ln_fG         -1.3624      1.274     -1.069      0.285      -3.860       1.135\n",
      "ln_R          10.9354      1.973      5.541      0.000       7.068      14.803\n",
      "ln_RBI         1.2999      0.902      1.441      0.150      -0.468       3.068\n",
      "ln_DP          0.5119      0.459      1.116      0.264      -0.387       1.411\n",
      "ln_since       2.0812      0.453      4.593      0.000       1.193       2.969\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.52 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "\n",
      "F1 on train data: 72.13\n",
      "F1 on test data:  84.62\n"
     ]
    }
   ],
   "source": [
    "##  list of exogenous variables for regression model\n",
    "exog = []\n",
    "# exog = exog + ['ln_bG']\n",
    "exog = exog + ['ln_fG']\n",
    "exog = exog + ['ln_R']\n",
    "# exog = exog + ['ln_H']\n",
    "# exog = exog + ['ln_2B']\n",
    "# exog = exog + ['ln_3B']\n",
    "# exog = exog + ['ln_HR']\n",
    "exog = exog + ['ln_RBI']\n",
    "# exog = exog + ['ln_bSB']\n",
    "# exog = exog + ['ln_PO']\n",
    "# exog = exog + ['ln_A']\n",
    "exog = exog + ['ln_DP']\n",
    "# exog = exog + ['ln_nu_sns']\n",
    "exog = exog + ['ln_since']\n",
    "\n",
    "##  regression formula\n",
    "m01a_fmla = 'induct~'\n",
    "m01a_fmla = m01a_fmla + '+'.join(exog)\n",
    "\n",
    "##  run logit\n",
    "print()\n",
    "print('logit results below on fit to WHOLE dataset')\n",
    "print()\n",
    "m01a_lgt = smf.logit(m01a_fmla,data=df).fit()\n",
    "print(m01a_lgt.summary())\n",
    "\n",
    "## instantiate standard logit model\n",
    "logit_mdl = LogisticRegression(penalty='none',max_iter=500) \n",
    "\n",
    "##  add to list of models\n",
    "# models['logit_mdl'] = logit_mdl\n",
    "\n",
    "## fit the model\n",
    "#logit_mdl.fit(X_train[exog], y_train)\n",
    "logit_mdl.fit(X_smote[exog], y_smote)\n",
    "\n",
    "## generate predictions\n",
    "y_hat_train = logit_mdl.predict(X_train[exog])\n",
    "y_hat_pred  = logit_mdl.predict(X_test[exog])\n",
    "\n",
    "## calculate F1 scores\n",
    "fone_train = f1_score(y_train,y_hat_train) * 100\n",
    "fone_test  = f1_score(y_test, y_hat_pred)  * 100\n",
    "print()\n",
    "print('F1 on train data: {:.2f}'.format(fone_train))\n",
    "print('F1 on test data:  {:.2f}'.format(fone_test))\n",
    "\n",
    "## calculate F1 scores\n",
    "acc_train = accuracy_score(y_train,y_hat_train) * 100\n",
    "acc_test  = accuracy_score(y_test, y_hat_pred)  * 100\n",
    "# print()\n",
    "# print('Acc on train data: {:.2f}'.format(acc_train))\n",
    "# print('Acc on test data:  {:.2f}'.format(acc_test))\n",
    "\n",
    "## clean up\n",
    "del y_hat_train, y_hat_pred, fone_train, fone_test, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
