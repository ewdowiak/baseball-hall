{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predicting Baseball Hall of Fame Induction\n",
    "\n",
    "###  Analysis 1 of 4 -- Pitchers\n",
    "\n",
    "####  Eryk Wdowiak and Ken Hoffman\n",
    "\n",
    "data from Lahman Baseball Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import VotingClassifier ##, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  the little date string\n",
    "dt_str = '2020-08-19a'\n",
    "\n",
    "# get the dataframes from pickle files\n",
    "pitchers_df = pickle.load(open('pitchers-df_'+ dt_str +'.p','rb'))\n",
    "# catchers_df = pickle.load(open('catchers-df_'+ dt_str +'.p','rb'))\n",
    "# infielders_df = pickle.load(open('infielders-df_'+ dt_str +'.p','rb'))\n",
    "# outfielders_df = pickle.load(open('outfielders-df_'+ dt_str +'.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bball_log(x):\n",
    "    warnings.filterwarnings('ignore',category=RuntimeWarning)\n",
    "    return np.where(x<1,0,np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies(x, prefix):\n",
    "    return pd.get_dummies(x, prefix = prefix, drop_first = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improve features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  take logs\n",
    "pitchers_df['ln_G'] = bball_log(pitchers_df['G'])\n",
    "pitchers_df['ln_G_ps'] = bball_log(pitchers_df['G_ps'])\n",
    "pitchers_df['ln_W'] = bball_log(pitchers_df['W'])\n",
    "pitchers_df['ln_W_ps'] = bball_log(pitchers_df['W_ps'])\n",
    "pitchers_df['ln_SO'] = bball_log(pitchers_df['SO'])\n",
    "pitchers_df['ln_SO_ps'] = bball_log(pitchers_df['SO_ps'])\n",
    "pitchers_df['ln_ER'] = bball_log(pitchers_df['ER'])\n",
    "pitchers_df['ln_ER_ps'] = bball_log(pitchers_df['ER_ps'])\n",
    "pitchers_df['ln_IPouts'] = bball_log(pitchers_df['IPouts'])\n",
    "pitchers_df['ln_IPouts_ps'] = bball_log(pitchers_df['IPouts_ps'])\n",
    "\n",
    "##  not sure about this one\n",
    "pitchers_df['ln_nu_sns'] = bball_log(pitchers_df['nu_sns'])\n",
    "\n",
    "##  years since retirement\n",
    "pitchers_df['since_lst'] = 2018 - pitchers_df['lst_sn']\n",
    "pitchers_df['ln_since'] = np.log(pitchers_df['since_lst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  I do NOT think it's a good idea to use team dummies.\n",
    "##  The estimated regression coefficients for a team with one (or few) \n",
    "##  inductees could be large, while the true values are zero.\n",
    "##  \n",
    "##  Nonetheless, to include team dummies, you would add the line:\n",
    "##  ##  exog = exog + tm_incl\n",
    "##  to the model formulation below.\n",
    "\n",
    "##  create dummies for teams\n",
    "team_dums = dummies(pitchers_df['teamID'],'tm')\n",
    "# pitchers_df = pitchers_df.join(team_dums)\n",
    "\n",
    "##  just need the list\n",
    "team_cols = team_dums.columns\n",
    "\n",
    "##  exclude teams without pitcher inductees\n",
    "tm_excl = ['tm_ANA', 'tm_ARI', 'tm_BRO', 'tm_BSN', 'tm_CIN', 'tm_COL', \n",
    "           'tm_FLO', 'tm_HOU', 'tm_KC1', 'tm_KCA', 'tm_LAA', 'tm_MIL', \n",
    "           'tm_ML4', 'tm_MON', 'tm_NY1', 'tm_PHA', 'tm_PIT', 'tm_SLA', \n",
    "           'tm_TBA', 'tm_TEX', 'tm_TOR', 'tm_WAS', 'tm_WS1', 'tm_WS2']\n",
    "\n",
    "##  make everything relative to the New York Yankees\n",
    "tm_excl = tm_excl + ['tm_NYA']\n",
    "\n",
    "##  columns to include\n",
    "tm_incl = [dum for dum in team_cols if dum not in tm_excl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  prepare training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  train_test_split()\n",
    "XX = pitchers_df.drop(columns=['induct'])\n",
    "yy = pitchers_df['induct']\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, yy, random_state=42,\n",
    "                                                    test_size=400)\n",
    "\n",
    "##  what should be SMOTE's ratio of minority to majority?\n",
    "smote_ratio = 0.25\n",
    "\n",
    "##  Fit SMOTE to training data\n",
    "Xs_train = X_train.drop(columns=['playerID','teamID'])\n",
    "Xs_test  = X_test.drop(columns=['playerID','teamID'])\n",
    "X_smote, y_smote = SMOTE(sampling_strategy = smote_ratio,\n",
    "                         random_state = 19).fit_sample(Xs_train, y_train)\n",
    "\n",
    "##  recreate the old layout\n",
    "pitchers_train = X_train.join(y_train)\n",
    "pitchers_test = X_test.join(y_test)\n",
    "pitchers_smote = X_smote.join(y_smote)\n",
    "\n",
    "##  clean up!\n",
    "del XX, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  estimation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  keep a list of models\n",
    "# models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "logit results below on fit to WHOLE dataset\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.049565\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 induct   No. Observations:                  900\n",
      "Model:                          Logit   Df Residuals:                      893\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Wed, 19 Aug 2020   Pseudo R-squ.:                  0.7109\n",
      "Time:                        16:34:15   Log-Likelihood:                -44.609\n",
      "converged:                       True   LL-Null:                       -154.31\n",
      "                                        LLR p-value:                 1.390e-44\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -60.0081     18.796     -3.193      0.001     -96.847     -23.169\n",
      "ln_G           3.5331      1.063      3.323      0.001       1.449       5.617\n",
      "ln_W          11.2482      3.999      2.813      0.005       3.411      19.086\n",
      "ln_SO          3.1603      1.517      2.083      0.037       0.187       6.133\n",
      "ln_ER        -13.4073      3.634     -3.689      0.000     -20.530      -6.284\n",
      "ln_IPouts      4.5674      6.054      0.754      0.451      -7.298      16.433\n",
      "ln_since       1.6173      0.621      2.605      0.009       0.400       2.834\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.58 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "\n",
      "F1 on train data: 82.14\n",
      "F1 on test data:  64.29\n",
      "\n",
      "Acc on train data: 98.00\n",
      "Acc on test data:  97.50\n"
     ]
    }
   ],
   "source": [
    "##  list of exogenous variables for regression model\n",
    "exog = ['ln_G']\n",
    "# exog = exog + ['ln_G_ps']\n",
    "exog = exog + ['ln_W']\n",
    "# exog = exog + ['ln_W_ps']\n",
    "exog = exog + ['ln_SO']\n",
    "# exog = exog + ['ln_SO_ps']\n",
    "exog = exog + ['ln_ER']\n",
    "# exog = exog + ['ln_ER_ps']\n",
    "exog = exog + ['ln_IPouts']\n",
    "# exog = exog + ['ln_IPouts_ps']\n",
    "# exog = exog + ['ln_nu_sns']\n",
    "exog = exog + ['ln_since']\n",
    "\n",
    "##  regression formula\n",
    "m01a_fmla = 'induct~'\n",
    "m01a_fmla = m01a_fmla + '+'.join(exog)\n",
    "\n",
    "##  run logit\n",
    "print()\n",
    "print('logit results below on fit to WHOLE dataset')\n",
    "print()\n",
    "m01a_lgt = smf.logit(m01a_fmla,data=pitchers_df).fit()\n",
    "print(m01a_lgt.summary())\n",
    "\n",
    "## instantiate standard logit model\n",
    "logit_mdl = LogisticRegression(penalty='none',max_iter=500) \n",
    "\n",
    "##  add to list of models\n",
    "# models['logit_mdl'] = logit_mdl\n",
    "\n",
    "## fit the model\n",
    "#logit_mdl.fit(X_train[exog], y_train)\n",
    "logit_mdl.fit(X_smote[exog], y_smote)\n",
    "\n",
    "## generate predictions\n",
    "y_hat_train = logit_mdl.predict(X_train[exog])\n",
    "y_hat_pred  = logit_mdl.predict(X_test[exog])\n",
    "\n",
    "## calculate F1 scores\n",
    "fone_train = f1_score(y_train,y_hat_train) * 100\n",
    "fone_test  = f1_score(y_test, y_hat_pred)  * 100\n",
    "print()\n",
    "print('F1 on train data: {:.2f}'.format(fone_train))\n",
    "print('F1 on test data:  {:.2f}'.format(fone_test))\n",
    "\n",
    "## calculate F1 scores\n",
    "acc_train = accuracy_score(y_train,y_hat_train) * 100\n",
    "acc_test  = accuracy_score(y_test, y_hat_pred)  * 100\n",
    "print()\n",
    "print('Acc on train data: {:.2f}'.format(acc_train))\n",
    "print('Acc on test data:  {:.2f}'.format(acc_test))\n",
    "\n",
    "## clean up\n",
    "del y_hat_train, y_hat_pred, fone_train, fone_test, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
